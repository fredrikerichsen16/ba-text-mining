{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import pathlib\n",
    "import gensim\n",
    "import itertools\n",
    "from helpers import preprocess_tweets\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_goemotion = pd.read_csv(os.path.abspath('data/GoEmotions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldnâ€™t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldnâ€™t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_goemotion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC Emotion Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.abspath('data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'), 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         word, emotion, \n",
    "\n",
    "nrc_df = pd.read_csv(os.path.abspath('data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'), sep='\\t', header=0, names=['word', 'emotion', 'intensity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitcoin Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hw/8skdg8jd63946x0yqr3hmqcw0000gn/T/ipykernel_8996/2347101721.py:1: DtypeWarning: Columns (5,6,7,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  btc_df = pd.read_csv(os.path.abspath('data/Bitcoin_tweets.csv'))\n"
     ]
    }
   ],
   "source": [
    "btc_df = pd.read_csv(os.path.abspath('data/Bitcoin_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeSota Wilson</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>Biz Consultant, real estate, fintech, startups...</td>\n",
       "      <td>2009-04-26 20:05:09</td>\n",
       "      <td>8534.0</td>\n",
       "      <td>7605</td>\n",
       "      <td>4838</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:59:04</td>\n",
       "      <td>Blue Ridge Bank shares halted by NYSE after #b...</td>\n",
       "      <td>['bitcoin']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CryptoND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ðŸ˜Ž BITCOINLIVE is a Dutch platform aimed at inf...</td>\n",
       "      <td>2019-10-17 20:12:10</td>\n",
       "      <td>6769.0</td>\n",
       "      <td>1532</td>\n",
       "      <td>25483</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:58:48</td>\n",
       "      <td>ðŸ˜Ž Today, that's this #Thursday, we will do a \"...</td>\n",
       "      <td>['Thursday', 'Btc', 'wallet', 'security']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tdlmatias</td>\n",
       "      <td>London, England</td>\n",
       "      <td>IM Academy : The best #forex, #SelfEducation, ...</td>\n",
       "      <td>2014-11-10 10:50:37</td>\n",
       "      <td>128.0</td>\n",
       "      <td>332</td>\n",
       "      <td>924</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:48</td>\n",
       "      <td>Guys evening, I have read this article about B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crypto is the future</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I will post a lot of buying signals for BTC tr...</td>\n",
       "      <td>2019-09-28 16:48:12</td>\n",
       "      <td>625.0</td>\n",
       "      <td>129</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:33</td>\n",
       "      <td>$BTC A big chance in a billion! Price: \\487264...</td>\n",
       "      <td>['Bitcoin', 'FX', 'BTC', 'crypto']</td>\n",
       "      <td>dlvr.it</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alex Kirchmaier ðŸ‡¦ðŸ‡¹ðŸ‡¸ðŸ‡ª #FactsSuperspreader</td>\n",
       "      <td>Europa</td>\n",
       "      <td>Co-founder @RENJERJerky | Forbes 30Under30 | I...</td>\n",
       "      <td>2016-02-03 13:15:55</td>\n",
       "      <td>1249.0</td>\n",
       "      <td>1472</td>\n",
       "      <td>10482</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:06</td>\n",
       "      <td>This network is secured by 9 508 nodes as of t...</td>\n",
       "      <td>['BTC']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_name    user_location  \\\n",
       "0                             DeSota Wilson      Atlanta, GA   \n",
       "1                                  CryptoND              NaN   \n",
       "2                                 Tdlmatias  London, England   \n",
       "3                      Crypto is the future              NaN   \n",
       "4  Alex Kirchmaier ðŸ‡¦ðŸ‡¹ðŸ‡¸ðŸ‡ª #FactsSuperspreader           Europa   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Biz Consultant, real estate, fintech, startups...  2009-04-26 20:05:09   \n",
       "1  ðŸ˜Ž BITCOINLIVE is a Dutch platform aimed at inf...  2019-10-17 20:12:10   \n",
       "2  IM Academy : The best #forex, #SelfEducation, ...  2014-11-10 10:50:37   \n",
       "3  I will post a lot of buying signals for BTC tr...  2019-09-28 16:48:12   \n",
       "4  Co-founder @RENJERJerky | Forbes 30Under30 | I...  2016-02-03 13:15:55   \n",
       "\n",
       "   user_followers user_friends user_favourites user_verified  \\\n",
       "0          8534.0         7605            4838         False   \n",
       "1          6769.0         1532           25483         False   \n",
       "2           128.0          332             924         False   \n",
       "3           625.0          129              14         False   \n",
       "4          1249.0         1472           10482         False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2021-02-10 23:59:04  Blue Ridge Bank shares halted by NYSE after #b...   \n",
       "1  2021-02-10 23:58:48  ðŸ˜Ž Today, that's this #Thursday, we will do a \"...   \n",
       "2  2021-02-10 23:54:48  Guys evening, I have read this article about B...   \n",
       "3  2021-02-10 23:54:33  $BTC A big chance in a billion! Price: \\487264...   \n",
       "4  2021-02-10 23:54:06  This network is secured by 9 508 nodes as of t...   \n",
       "\n",
       "                                    hashtags               source is_retweet  \n",
       "0                                ['bitcoin']      Twitter Web App      False  \n",
       "1  ['Thursday', 'Btc', 'wallet', 'security']  Twitter for Android      False  \n",
       "2                                        NaN      Twitter Web App      False  \n",
       "3         ['Bitcoin', 'FX', 'BTC', 'crypto']              dlvr.it      False  \n",
       "4                                    ['BTC']      Twitter Web App      False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sentence Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings\n",
    "- PoS\n",
    "- Positive/Neutral word 000110110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing BertForTextRepresentation: ['roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'lm_head.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTextRepresentation were not initialized from the model checkpoint at roberta-base and are newly initialized: ['encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = RepresentationModel(\n",
    "#     model_type=\"bert\",\n",
    "#     model_name=\"bert-base-uncased\",\n",
    "#     use_cuda=False\n",
    "# )\n",
    "\n",
    "model = RepresentationModel(\n",
    "    model_type=\"roberta\",\n",
    "    model_name=\"roberta-base\",\n",
    "    use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = os.path.abspath('data/GoogleNews-vectors-negative300.bin')\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word2vec_from_scratch(sentences):\n",
    "    # return gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "    return sentences\n",
    "\n",
    "def encode_bert(sentences, model):\n",
    "    return model.encode_sentences(sentences, combine_strategy=None)\n",
    "\n",
    "def encode_word2vec(sentences):\n",
    "    model = word_embedding_model\n",
    "\n",
    "    sentences_vector = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sent_vector = []\n",
    "\n",
    "        for token in nlp(sentence):\n",
    "            if token.is_stop:\n",
    "                continue\n",
    "            \n",
    "            if token.text in model:\n",
    "                sent_vector.append(model[token.text])\n",
    "            else:\n",
    "                sent_vector.append([0] * 300)\n",
    "        \n",
    "        sentences_vector.append(sent_vector)\n",
    "    \n",
    "    return sentences_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df_goemotion.text)[:200]\n",
    "\n",
    "goemotions_sentences_embedded = encode_word2vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    vector = []\n",
    "\n",
    "    for token in nlp(sentence):\n",
    "        dictionary = {}\n",
    "\n",
    "        if not token.is_stop:\n",
    "            dictionary['pos'] = token.pos_\n",
    "            \n",
    "            df_ = nrc_df[nrc_df['word'] == token.lemma_]\n",
    "\n",
    "            for emotion, intensity in zip(df_.emotion.to_list(), df_.intensity.to_list()):\n",
    "                dictionary[emotion] = intensity\n",
    "        \n",
    "            vector.append(dictionary)\n",
    "    \n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vectorizer = DictVectorizer()\n",
    "\n",
    "dict_vectorizer = dict_vectorizer.fit(list(itertools.chain.from_iterable(vectors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hw/8skdg8jd63946x0yqr3hmqcw0000gn/T/ipykernel_8996/2387272780.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ohe_vectors = np.array(ohe_vectors)\n"
     ]
    }
   ],
   "source": [
    "ohe_vectors = []\n",
    "\n",
    "for vector in vectors:\n",
    "    new_vector = dict_vectorizer.transform(vector).toarray()\n",
    "    ohe_vectors.append(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/fredrik/Documents/GitHub/ba-text-mining/final-assignment/main.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fredrik/Documents/GitHub/ba-text-mining/final-assignment/main.ipynb#ch0000056?line=0'>1</a>\u001b[0m ohe_vectors\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "ohe_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11359</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11361</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11362</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11363</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11364</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11365</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11368</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word       emotion  intensity\n",
       "11359  beautiful         anger          0\n",
       "11360  beautiful  anticipation          0\n",
       "11361  beautiful       disgust          0\n",
       "11362  beautiful          fear          0\n",
       "11363  beautiful           joy          1\n",
       "11364  beautiful      negative          0\n",
       "11365  beautiful      positive          1\n",
       "11366  beautiful       sadness          0\n",
       "11367  beautiful      surprise          0\n",
       "11368  beautiful         trust          0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrc_df[nrc_df['word'] == 'beautiful']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anger -> anger, annoyance\n",
    "anticipation\n",
    "disgust -> annoyance(?), disapproval, disgust\n",
    "fear -> embarrassment, fear, nervousness\n",
    "joy -> amusement, caring, excitement, gratitude, joy, love, optimism\n",
    "negative\n",
    "positive\n",
    "sadness -> disappointment, grief\n",
    "surprise -> realization\n",
    "trust -> admiration, approval\n",
    "\n",
    "none: confusion, curiosity, desire, pride\n",
    "\n",
    "relief\n",
    "remorse\n",
    "sadness\n",
    "surprise\n",
    "neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
       "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
       "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
       "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
       "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
       "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
       "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_goemotion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(btc_df.text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = preprocess_tweets(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue Ridge Bank shares halted by NYSE after #bitcoin ATM announcement  @userâ€¦ ',\n",
       " 'ðŸ˜Ž Today, that\\'s this #Thursday, we will do a \"ðŸŽ¬ Take 2\" with our friend @user, #Btc #wallet #security expeâ€¦ ',\n",
       " 'Guys evening, I have read this article about BTC and would like to share with you all - â€¦ ',\n",
       " '$BTC A big chance in a billion! Price: \\\\4872644.0 (2021/02/11 08:51) #Bitcoin #FX #BTC #crypto',\n",
       " 'This network is secured by 9 508 nodes as of today. Soon, the biggest bears will recognise: #BTC in too big to failâ€¦ ',\n",
       " 'ðŸ’¹ Trade #Crypto on #Binance \\n\\nðŸ“Œ Enjoy #Cashback 10% of the Trading fee\\nðŸ“Œ Sign up link ðŸ‘‰ â€¦ ',\n",
       " \"&lt;'fire' &amp; 'man'&gt;\\n#Bitcoin #Crypto #BTC \",\n",
       " 'ðŸ”„ Prices update in $EUR (1 hour):\\n\\n$BTC   - 37082.1 â‚¬  (-0.51 %)\\n$ETH   - 1441.59 â‚¬  (+0.21 %)\\n$XRP   - 0.42 â‚¬    (â€¦ ',\n",
       " '#BTC #Bitcoin #Ethereum #ETH #Crypto #cryptotrading \\n\\n$RSR I know i told you guys the target was $0.060, i know weâ€¦ ',\n",
       " '.@Teslaâ€™s #bitcoin investment is revolutionary for #crypto but other firms may not do the same just yet - @userâ€¦ ']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(nrc_df[nrc_df['word'] == 'crying'].emotion.to_list())\n",
    "print(nrc_df[nrc_df['word'] == 'crying'].intensity.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "436a260425d2d1f41a03db831630dbba59f84586031c56b41ec9dd7831ec3a5e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('TextMining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
