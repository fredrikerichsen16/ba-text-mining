{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TextMining/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import pathlib\n",
    "import gensim\n",
    "import itertools\n",
    "from helpers import preprocess_tweets, preprocess_tweet\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download links for data\n",
    "\n",
    "- Bitcoin_tweeets.csv -> https://www.kaggle.com/kaushiksuresh147/bitcoin-tweets\n",
    "- GoEmotions.csv -> https://www.kaggle.com/datasets/debarshichanda/goemotions\n",
    "- sentiment140 -> http://help.sentiment140.com/for-students/\n",
    "- GoogleNews-vectors-etc... (Word2Vec) -> Canvas(?)\n",
    "- NRC Emotion Lexicon -> https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
    "- nrc_emotion_lexicon_dict -> Google Drive\n",
    "- BTC-USD -> Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90th percentile length (after removing stopwords and punctuation) was about 14 when I tested, so this is a good cutoff (99th percentile = 18)\n",
    "MAX_SENTENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_train = pd.read_csv(\n",
    "    os.path.abspath('data/sentiment140-train.csv'), \n",
    "    encoding='ISO-8859-1', \n",
    "    header=None, \n",
    "    names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
    "    usecols=['polarity', 'text']\n",
    ")\n",
    "\n",
    "df_sf_test = pd.read_csv(\n",
    "    os.path.abspath('data/sentiment140-test.csv'), \n",
    "    encoding='ISO-8859-1', \n",
    "    header=None, \n",
    "    names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
    "    usecols=['polarity', 'text']\n",
    ")\n",
    "\n",
    "df_sf_train['polarity'] = df_sf_train['polarity'].replace(4, 1)\n",
    "df_sf_test['polarity'] = df_sf_train['polarity'].replace(4, 1)\n",
    "\n",
    "df_sf_train['text'] = df_sf_train['text'].apply(preprocess_tweet)\n",
    "df_sf_test['text'] = df_sf_test['text'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     polarity                                               text\n",
       "493         0  Ask Programming: LaTeX or InDesign?: submitted...\n",
       "494         0  On that note, I hate Word. I hate Pages. I hat...\n",
       "495         0  Ahhh... back in a *real* text editing environm...\n",
       "496         0  Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
       "497         0  Reading the tweets coming out of Iran... The w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sf_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_goemotion = pd.read_csv(os.path.abspath('data/GoEmotions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldnâ€™t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldnâ€™t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_goemotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211225"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_goemotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gosentiment = df_goemotion.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emotions = ['admiration', 'amusement', 'approval', 'caring', 'curiosity', 'desire', 'excitement', 'gratitude', 'joy', 'love', 'optimism', 'pride', 'relief']\n",
    "negative_emotions = ['anger', 'annoyance', 'disappointment', 'disapproval', 'disgust', 'fear', 'grief', 'nervousness', 'remorse', 'sadness']\n",
    "neutral_emotions = ['neutral', 'embarrassment', 'confusion', 'realization', 'surprise']\n",
    "\n",
    "df_gosentiment['Positive'] = df_gosentiment[positive_emotions].sum(axis=1).apply(lambda x: min(1, x))\n",
    "df_gosentiment['Negative'] = df_gosentiment[negative_emotions].sum(axis=1).apply(lambda x: min(1, x))\n",
    "df_gosentiment['Neutral'] = df_gosentiment[neutral_emotions].sum(axis=1).apply(lambda x: min(1, x))\n",
    "\n",
    "also_drop_columns = ['subreddit', 'id', 'link_id', 'author', 'parent_id', 'rater_id']\n",
    "\n",
    "df_gosentiment.drop(labels=positive_emotions + negative_emotions + neutral_emotions + also_drop_columns, axis=1, inplace=True)\n",
    "\n",
    "df_gosentiment['Polarity'] = 0\n",
    "\n",
    "for index, row in df_gosentiment.iterrows():\n",
    "    if row['Positive'] == 1:\n",
    "        df_gosentiment.at[index, 'Polarity'] = 1\n",
    "    elif row['Negative'] == 1:\n",
    "        df_gosentiment.at[index, 'Polarity'] = -1\n",
    "\n",
    "df_gosentiment = df_gosentiment.astype({\n",
    "    'Positive': 'int',\n",
    "    'Negative': 'int',\n",
    "    'Neutral': 'int',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldnâ€™t be a grouping category I...</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Right? Considering itâ€™s such an important docu...</td>\n",
       "      <td>1.548280e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>He isn't as big, but he's still quite popular....</td>\n",
       "      <td>1.546320e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>That's crazy; I went to a super [RELIGION] hig...</td>\n",
       "      <td>1.546536e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>that's adorable asf</td>\n",
       "      <td>1.548764e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Sponge Blurb Pubs Quaw Haha GURR ha AAa!\" fin...</td>\n",
       "      <td>1.546984e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I have, and now that you mention it, I think t...</td>\n",
       "      <td>1.546658e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I wanted to downvote this, but it's not your f...</td>\n",
       "      <td>1.547580e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BUT IT'S HER TURN! /s</td>\n",
       "      <td>1.548720e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>That is odd.</td>\n",
       "      <td>1.547736e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Build a wall? /jk</td>\n",
       "      <td>1.547208e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I appreciate it, that's good to know. I hope I...</td>\n",
       "      <td>1.546372e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>One time my 1 stopped right in 91st, I was abl...</td>\n",
       "      <td>1.546881e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Well then Iâ€™d say you have a pretty good chanc...</td>\n",
       "      <td>1.546963e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Pretty much every Punjabi dude I've met.</td>\n",
       "      <td>1.546613e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For extra measure tape it right by your crotch...</td>\n",
       "      <td>1.546641e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   created_utc  \\\n",
       "0                                     That game hurt.  1.548381e+09   \n",
       "1    >sexuality shouldnâ€™t be a grouping category I...  1.548084e+09   \n",
       "2      You do right, if you don't care then fuck 'em!  1.546428e+09   \n",
       "3                                  Man I love reddit.  1.547965e+09   \n",
       "4   [NAME] was nowhere near them, he was by the Fa...  1.546669e+09   \n",
       "5   Right? Considering itâ€™s such an important docu...  1.548280e+09   \n",
       "6   He isn't as big, but he's still quite popular....  1.546320e+09   \n",
       "7   That's crazy; I went to a super [RELIGION] hig...  1.546536e+09   \n",
       "8                                 that's adorable asf  1.548764e+09   \n",
       "9   \"Sponge Blurb Pubs Quaw Haha GURR ha AAa!\" fin...  1.546984e+09   \n",
       "10  I have, and now that you mention it, I think t...  1.546658e+09   \n",
       "11  I wanted to downvote this, but it's not your f...  1.547580e+09   \n",
       "12                              BUT IT'S HER TURN! /s  1.548720e+09   \n",
       "13                                       That is odd.  1.547736e+09   \n",
       "14                                  Build a wall? /jk  1.547208e+09   \n",
       "15  I appreciate it, that's good to know. I hope I...  1.546372e+09   \n",
       "16  One time my 1 stopped right in 91st, I was abl...  1.546881e+09   \n",
       "17  Well then Iâ€™d say you have a pretty good chanc...  1.546963e+09   \n",
       "18           Pretty much every Punjabi dude I've met.  1.546613e+09   \n",
       "19  For extra measure tape it right by your crotch...  1.546641e+09   \n",
       "\n",
       "    example_very_unclear  Positive  Negative  Neutral  Polarity  \n",
       "0                  False         0         1        0        -1  \n",
       "1                   True         0         0        0         0  \n",
       "2                  False         0         0        1         0  \n",
       "3                  False         1         0        0         1  \n",
       "4                  False         0         0        1         0  \n",
       "5                  False         1         0        0         1  \n",
       "6                  False         0         1        0        -1  \n",
       "7                  False         1         0        0         1  \n",
       "8                  False         1         0        0         1  \n",
       "9                  False         1         0        0         1  \n",
       "10                 False         0         0        1         0  \n",
       "11                 False         0         1        0        -1  \n",
       "12                 False         0         0        1         0  \n",
       "13                 False         0         1        0        -1  \n",
       "14                 False         0         0        1         0  \n",
       "15                 False         1         0        0         1  \n",
       "16                 False         0         0        1         0  \n",
       "17                 False         0         0        1         0  \n",
       "18                 False         1         0        0         1  \n",
       "19                 False         0         1        0        -1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gosentiment.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC Emotion Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.abspath('data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'), 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         word, emotion, \n",
    "\n",
    "nrc_df = pd.read_csv(os.path.abspath('data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'), sep='\\t', header=0, names=['word', 'emotion', 'intensity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrc_dict = {}\n",
    "\n",
    "# # Iterate over nrc_df\n",
    "# for index, row in nrc_df.iterrows():\n",
    "#     # Get the word and emotion\n",
    "#     word = row['word']\n",
    "#     emotion = row['emotion']\n",
    "#     intensity = row['intensity']\n",
    "#     # If the word is not in the dict yet\n",
    "#     if word not in nrc_dict:\n",
    "#         # Initialize the word in the dict\n",
    "#         nrc_dict[word] = []\n",
    "#     # Add the emotion to the word\n",
    "#     if intensity == 1:\n",
    "#         nrc_dict[word].append(emotion)\n",
    "\n",
    "# # Writee nrc_dict to file\n",
    "# with open(os.path.abspath('data/nrc_emotion_lexicon_dict.json'), 'w') as f:\n",
    "#     f.write(json.dumps(nrc_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_dict = json.load(open(os.path.abspath('data/nrc_emotion_lexicon_dict.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitcoin Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hw/8skdg8jd63946x0yqr3hmqcw0000gn/T/ipykernel_25001/2347101721.py:1: DtypeWarning: Columns (5,6,7,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  btc_df = pd.read_csv(os.path.abspath('data/Bitcoin_tweets.csv'))\n"
     ]
    }
   ],
   "source": [
    "btc_df = pd.read_csv(os.path.abspath('data/Bitcoin_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeSota Wilson</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>Biz Consultant, real estate, fintech, startups...</td>\n",
       "      <td>2009-04-26 20:05:09</td>\n",
       "      <td>8534.0</td>\n",
       "      <td>7605</td>\n",
       "      <td>4838</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:59:04</td>\n",
       "      <td>Blue Ridge Bank shares halted by NYSE after #b...</td>\n",
       "      <td>['bitcoin']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CryptoND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ðŸ˜Ž BITCOINLIVE is a Dutch platform aimed at inf...</td>\n",
       "      <td>2019-10-17 20:12:10</td>\n",
       "      <td>6769.0</td>\n",
       "      <td>1532</td>\n",
       "      <td>25483</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:58:48</td>\n",
       "      <td>ðŸ˜Ž Today, that's this #Thursday, we will do a \"...</td>\n",
       "      <td>['Thursday', 'Btc', 'wallet', 'security']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tdlmatias</td>\n",
       "      <td>London, England</td>\n",
       "      <td>IM Academy : The best #forex, #SelfEducation, ...</td>\n",
       "      <td>2014-11-10 10:50:37</td>\n",
       "      <td>128.0</td>\n",
       "      <td>332</td>\n",
       "      <td>924</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:48</td>\n",
       "      <td>Guys evening, I have read this article about B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crypto is the future</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I will post a lot of buying signals for BTC tr...</td>\n",
       "      <td>2019-09-28 16:48:12</td>\n",
       "      <td>625.0</td>\n",
       "      <td>129</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:33</td>\n",
       "      <td>$BTC A big chance in a billion! Price: \\487264...</td>\n",
       "      <td>['Bitcoin', 'FX', 'BTC', 'crypto']</td>\n",
       "      <td>dlvr.it</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alex Kirchmaier ðŸ‡¦ðŸ‡¹ðŸ‡¸ðŸ‡ª #FactsSuperspreader</td>\n",
       "      <td>Europa</td>\n",
       "      <td>Co-founder @RENJERJerky | Forbes 30Under30 | I...</td>\n",
       "      <td>2016-02-03 13:15:55</td>\n",
       "      <td>1249.0</td>\n",
       "      <td>1472</td>\n",
       "      <td>10482</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 23:54:06</td>\n",
       "      <td>This network is secured by 9 508 nodes as of t...</td>\n",
       "      <td>['BTC']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_name    user_location  \\\n",
       "0                             DeSota Wilson      Atlanta, GA   \n",
       "1                                  CryptoND              NaN   \n",
       "2                                 Tdlmatias  London, England   \n",
       "3                      Crypto is the future              NaN   \n",
       "4  Alex Kirchmaier ðŸ‡¦ðŸ‡¹ðŸ‡¸ðŸ‡ª #FactsSuperspreader           Europa   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Biz Consultant, real estate, fintech, startups...  2009-04-26 20:05:09   \n",
       "1  ðŸ˜Ž BITCOINLIVE is a Dutch platform aimed at inf...  2019-10-17 20:12:10   \n",
       "2  IM Academy : The best #forex, #SelfEducation, ...  2014-11-10 10:50:37   \n",
       "3  I will post a lot of buying signals for BTC tr...  2019-09-28 16:48:12   \n",
       "4  Co-founder @RENJERJerky | Forbes 30Under30 | I...  2016-02-03 13:15:55   \n",
       "\n",
       "   user_followers user_friends user_favourites user_verified  \\\n",
       "0          8534.0         7605            4838         False   \n",
       "1          6769.0         1532           25483         False   \n",
       "2           128.0          332             924         False   \n",
       "3           625.0          129              14         False   \n",
       "4          1249.0         1472           10482         False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2021-02-10 23:59:04  Blue Ridge Bank shares halted by NYSE after #b...   \n",
       "1  2021-02-10 23:58:48  ðŸ˜Ž Today, that's this #Thursday, we will do a \"...   \n",
       "2  2021-02-10 23:54:48  Guys evening, I have read this article about B...   \n",
       "3  2021-02-10 23:54:33  $BTC A big chance in a billion! Price: \\487264...   \n",
       "4  2021-02-10 23:54:06  This network is secured by 9 508 nodes as of t...   \n",
       "\n",
       "                                    hashtags               source is_retweet  \n",
       "0                                ['bitcoin']      Twitter Web App      False  \n",
       "1  ['Thursday', 'Btc', 'wallet', 'security']  Twitter for Android      False  \n",
       "2                                        NaN      Twitter Web App      False  \n",
       "3         ['Bitcoin', 'FX', 'BTC', 'crypto']              dlvr.it      False  \n",
       "4                                    ['BTC']      Twitter Web App      False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sentence Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings\n",
    "- PoS\n",
    "- Positive/Neutral word 000110110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing BertForTextRepresentation: ['roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'lm_head.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTextRepresentation were not initialized from the model checkpoint at roberta-base and are newly initialized: ['encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = RepresentationModel(\n",
    "#     model_type=\"bert\",\n",
    "#     model_name=\"bert-base-uncased\",\n",
    "#     use_cuda=False\n",
    "# )\n",
    "\n",
    "model = RepresentationModel(\n",
    "    model_type=\"roberta\",\n",
    "    model_name=\"roberta-base\",\n",
    "    use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = os.path.abspath('data/GoogleNews-vectors-negative300.bin')\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word2vec_from_scratch(sentences):\n",
    "    # return gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "    return sentences\n",
    "\n",
    "def encode_bert(sentences, model):\n",
    "    return model.encode_sentences(sentences, combine_strategy=None)\n",
    "\n",
    "def encode_word2vec(sentences):\n",
    "    model = word_embedding_model\n",
    "\n",
    "    sentences_vector = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sent_vector = []\n",
    "\n",
    "        for token in nlp(sentence):\n",
    "            if token.is_stop:\n",
    "                continue\n",
    "            \n",
    "            if token.text in model:\n",
    "                sent_vector.append(model[token.text])\n",
    "            else:\n",
    "                sent_vector.append([0] * 300)\n",
    "        \n",
    "        if len(sent_vector) > MAX_SENTENCE_LENGTH:\n",
    "            sent_vector = sent_vector[:MAX_SENTENCE_LENGTH]\n",
    "        else:\n",
    "            sent_vector = sent_vector + [[0] * 300] * (MAX_SENTENCE_LENGTH - len(sent_vector))\n",
    "\n",
    "        sentences_vector.append(sent_vector)\n",
    "    \n",
    "    return np.array(sentences_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_other_features(sentences):\n",
    "    vectors = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        vector = []\n",
    "\n",
    "        for token in nlp(sentence):\n",
    "            dictionary = {}\n",
    "\n",
    "            if not (token.is_stop or token.is_punct):\n",
    "                dictionary['pos'] = token.pos_\n",
    "                \n",
    "                associated_emotions = nrc_dict.get(token.lemma_, [])\n",
    "\n",
    "                for emotion in associated_emotions:\n",
    "                    dictionary[emotion] = True\n",
    "                \n",
    "                vector.append(dictionary)\n",
    "\n",
    "        if len(vector) > MAX_SENTENCE_LENGTH:\n",
    "            vector = vector[:MAX_SENTENCE_LENGTH]\n",
    "        else:\n",
    "            vector = vector + [{}] * (MAX_SENTENCE_LENGTH - len(vector))\n",
    "\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    print(len(vectors))\n",
    "\n",
    "    dict_vectorizer = DictVectorizer()\n",
    "    dict_vectorizer = dict_vectorizer.fit(list(itertools.chain.from_iterable(vectors)))\n",
    "\n",
    "    encoded = []\n",
    "\n",
    "    for vector in vectors:\n",
    "        encoded.append(dict_vectorizer.transform(vector).toarray())\n",
    "    \n",
    "    return np.array(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Stanford 140 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "(500, 48, 300)\n",
      "(500, 48, 28)\n",
      "(500, 48, 328)\n",
      "(500, 15744)\n"
     ]
    }
   ],
   "source": [
    "sentences = list(df_sf_train.text)[:250] + list(df_sf_train.text)[-250:]\n",
    "\n",
    "sentences = preprocess_tweets(sentences)\n",
    "sentences_embedded = encode_word2vec(sentences)\n",
    "features_embedded = encode_other_features(sentences)\n",
    "\n",
    "# Combine sentences_embedded and features_embedded on the third dimension\n",
    "combined_embedded = np.concatenate((sentences_embedded, features_embedded), axis=2)\n",
    "\n",
    "combined_embedded_2d = combined_embedded.reshape(combined_embedded.shape[0], combined_embedded.shape[1] * combined_embedded.shape[2]) # combined_embedded.reshape(combined_embedded.shape[0], -1)\n",
    "\n",
    "print(sentences_embedded.shape)\n",
    "print(features_embedded.shape)\n",
    "print(combined_embedded.shape)\n",
    "print(combined_embedded_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_sf_train.polarity.to_list()[:250] + df_sf_train.polarity.to_list()[-250:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embedded_2d, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.50      0.55        46\n",
      "           4       0.63      0.74      0.68        54\n",
      "\n",
      "    accuracy                           0.63       100\n",
      "   macro avg       0.63      0.62      0.62       100\n",
      "weighted avg       0.63      0.63      0.62       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On GoSentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "(5000, 28, 300)\n",
      "(5000, 28, 28)\n",
      "(5000, 28, 328)\n",
      "(5000, 9184)\n"
     ]
    }
   ],
   "source": [
    "sentences = list(df_gosentiment.text)[:5000]\n",
    "\n",
    "# sentences = preprocess_tweets(sentences)\n",
    "sentences_embedded = encode_word2vec(sentences)\n",
    "features_embedded = encode_other_features(sentences)\n",
    "\n",
    "# Combine sentences_embedded and features_embedded on the third dimension\n",
    "combined_embedded = np.concatenate((sentences_embedded, features_embedded), axis=2)\n",
    "\n",
    "combined_embedded_2d = combined_embedded.reshape(combined_embedded.shape[0], combined_embedded.shape[1] * combined_embedded.shape[2]) # combined_embedded.reshape(combined_embedded.shape[0], -1)\n",
    "\n",
    "print(sentences_embedded.shape)\n",
    "print(features_embedded.shape)\n",
    "print(combined_embedded.shape)\n",
    "print(combined_embedded_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.35      0.42      0.38       226\n",
      "           0       0.45      0.42      0.43       354\n",
      "           1       0.56      0.53      0.54       420\n",
      "\n",
      "    accuracy                           0.47      1000\n",
      "   macro avg       0.45      0.46      0.45      1000\n",
      "weighted avg       0.47      0.47      0.47      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = df_gosentiment.Polarity.to_list()[:5000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embedded_2d, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "pred = svm.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader = df_sf_train.sample(frac=0.01).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader.text = df_vader.text.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "#Add VADER metrics to dataframe\n",
    "df_vader['compound'] = [analyzer.polarity_scores(v)['compound'] for v in df_vader['text']]\n",
    "df_vader['neg'] = [analyzer.polarity_scores(v)['neg'] for v in df_vader['text']]\n",
    "df_vader['neu'] = [analyzer.polarity_scores(v)['neu'] for v in df_vader['text']]\n",
    "df_vader['pos'] = [analyzer.polarity_scores(v)['pos'] for v in df_vader['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preeprocessing idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11359</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11361</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11362</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11363</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11364</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11365</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11368</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word       emotion  intensity\n",
       "11359  beautiful         anger          0\n",
       "11360  beautiful  anticipation          0\n",
       "11361  beautiful       disgust          0\n",
       "11362  beautiful          fear          0\n",
       "11363  beautiful           joy          1\n",
       "11364  beautiful      negative          0\n",
       "11365  beautiful      positive          1\n",
       "11366  beautiful       sadness          0\n",
       "11367  beautiful      surprise          0\n",
       "11368  beautiful         trust          0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrc_df[nrc_df['word'] == 'beautiful']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anger -> anger, annoyance\n",
    "anticipation\n",
    "disgust -> annoyance(?), disapproval, disgust\n",
    "fear -> embarrassment, fear, nervousness\n",
    "joy -> amusement, caring, excitement, gratitude, joy, love, optimism\n",
    "negative\n",
    "positive\n",
    "sadness -> disappointment, grief\n",
    "surprise -> realization\n",
    "trust -> admiration, approval\n",
    "\n",
    "none: confusion, curiosity, desire, pride\n",
    "\n",
    "relief\n",
    "remorse\n",
    "sadness\n",
    "surprise\n",
    "neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
       "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
       "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
       "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
       "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
       "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
       "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_goemotion.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "436a260425d2d1f41a03db831630dbba59f84586031c56b41ec9dd7831ec3a5e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('TextMining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
