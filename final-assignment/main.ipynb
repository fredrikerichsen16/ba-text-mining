{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import pathlib\n",
    "import time\n",
    "import gensim\n",
    "import itertools\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from helpers import preprocess_tweets, preprocess_tweet, preprocess_reddit, preprocess_reddits\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download links for data\n",
    "\n",
    "- Bitcoin_tweeets.csv -> https://www.kaggle.com/kaushiksuresh147/bitcoin-tweets\n",
    "- GoEmotions.csv -> https://www.kaggle.com/datasets/debarshichanda/goemotions\n",
    "- sentiment140 -> http://help.sentiment140.com/for-students/\n",
    "- GoogleNews-vectors-etc... (Word2Vec) -> Canvas(?)\n",
    "- NRC Emotion Lexicon -> https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
    "- nrc_emotion_lexicon_dict -> Google Drive\n",
    "- BTC-USD -> Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90th percentile length (after removing stopwords and punctuation) was about 14 when I tested, so this is a good cutoff (99th percentile = 18)\n",
    "MAX_SENTENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_train = pd.read_csv(\n",
    "    os.path.abspath('data/sentiment140-train.csv'), \n",
    "    encoding='ISO-8859-1', \n",
    "    header=None, \n",
    "    names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
    "    usecols=['polarity', 'text']\n",
    ")\n",
    "\n",
    "df_sf_test = pd.read_csv(\n",
    "    os.path.abspath('data/sentiment140-test.csv'), \n",
    "    encoding='ISO-8859-1', \n",
    "    header=None, \n",
    "    names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
    "    usecols=['polarity', 'text']\n",
    ")\n",
    "\n",
    "df_sf_train['polarity'] = df_sf_train['polarity'].replace(4, 1)\n",
    "df_sf_test['polarity'] = df_sf_train['polarity'].replace(4, 1)\n",
    "\n",
    "df_sf_train['text'] = df_sf_train['text'].apply(preprocess_tweet)\n",
    "df_sf_test['text'] = df_sf_test['text'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_goemotion = pd.read_csv(os.path.abspath('data/GoEmotions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_goemotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_goemotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gosentiment = df_goemotion.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emotions = ['admiration', 'amusement', 'approval', 'caring', 'curiosity', 'desire', 'excitement', 'gratitude', 'joy', 'love', 'optimism', 'pride', 'relief']\n",
    "negative_emotions = ['anger', 'annoyance', 'disappointment', 'disapproval', 'disgust', 'fear', 'grief', 'nervousness', 'remorse', 'sadness']\n",
    "neutral_emotions = ['neutral', 'embarrassment', 'confusion', 'realization', 'surprise']\n",
    "\n",
    "df_gosentiment['Positive'] = df_gosentiment[positive_emotions].sum(axis=1).apply(lambda x: min(1, x))\n",
    "df_gosentiment['Negative'] = df_gosentiment[negative_emotions].sum(axis=1).apply(lambda x: min(1, x))\n",
    "df_gosentiment['Neutral'] = df_gosentiment[neutral_emotions].sum(axis=1).apply(lambda x: min(1, x))\n",
    "\n",
    "also_drop_columns = ['subreddit', 'id', 'link_id', 'author', 'parent_id', 'rater_id']\n",
    "\n",
    "df_gosentiment.drop(labels=positive_emotions + negative_emotions + neutral_emotions + also_drop_columns, axis=1, inplace=True)\n",
    "\n",
    "df_gosentiment['Polarity'] = 0\n",
    "\n",
    "for index, row in df_gosentiment.iterrows():\n",
    "    if row['Positive'] == 1:\n",
    "        df_gosentiment.at[index, 'Polarity'] = 1\n",
    "    elif row['Negative'] == 1:\n",
    "        df_gosentiment.at[index, 'Polarity'] = -1\n",
    "\n",
    "df_gosentiment = df_gosentiment.astype({\n",
    "    'Positive': 'int',\n",
    "    'Negative': 'int',\n",
    "    'Neutral': 'int',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gosentiment.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC Emotion Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.abspath('data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'), 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         word, emotion, \n",
    "\n",
    "nrc_df = pd.read_csv(os.path.abspath('data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'), sep='\\t', header=0, names=['word', 'emotion', 'intensity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrc_dict = {}\n",
    "\n",
    "# # Iterate over nrc_df\n",
    "# for index, row in nrc_df.iterrows():\n",
    "#     # Get the word and emotion\n",
    "#     word = row['word']\n",
    "#     emotion = row['emotion']\n",
    "#     intensity = row['intensity']\n",
    "#     # If the word is not in the dict yet\n",
    "#     if word not in nrc_dict:\n",
    "#         # Initialize the word in the dict\n",
    "#         nrc_dict[word] = []\n",
    "#     # Add the emotion to the word\n",
    "#     if intensity == 1:\n",
    "#         nrc_dict[word].append(emotion)\n",
    "\n",
    "# # Writee nrc_dict to file\n",
    "# with open(os.path.abspath('data/nrc_emotion_lexicon_dict.json'), 'w') as f:\n",
    "#     f.write(json.dumps(nrc_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_dict = json.load(open(os.path.abspath('data/nrc_emotion_lexicon_dict.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitcoin Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df = pd.read_csv(os.path.abspath('data/Bitcoin_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sentence Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings\n",
    "- PoS\n",
    "- Positive/Neutral word 000110110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RepresentationModel(\n",
    "#     model_type=\"bert\",\n",
    "#     model_name=\"bert-base-uncased\",\n",
    "#     use_cuda=False\n",
    "# )\n",
    "\n",
    "model = RepresentationModel(\n",
    "    model_type=\"roberta\",\n",
    "    model_name=\"roberta-base\",\n",
    "    use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = os.path.abspath('data/GoogleNews-vectors-negative300.bin')\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=750000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word2vec_from_scratch(sentences):\n",
    "    # return gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "    return sentences\n",
    "\n",
    "def encode_bert(sentences, model):\n",
    "    return model.encode_sentences(sentences, combine_strategy=None)\n",
    "\n",
    "def encode_word2vec(sentences):\n",
    "    model = word_embedding_model\n",
    "\n",
    "    sentences_vector = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sent_vector = []\n",
    "\n",
    "        for token in nlp(sentence):\n",
    "            if token.is_stop or token.is_space or token.is_punct:\n",
    "                continue\n",
    "\n",
    "            lemma = token.lemma_.lower()\n",
    "            if lemma in model:\n",
    "                sent_vector.append(model[lemma])\n",
    "            elif token.text.lower() in model:\n",
    "                sent_vector.append(model[token.text.lower()])\n",
    "            else:\n",
    "                sent_vector.append([0] * 300)\n",
    "        \n",
    "        # if len(sent_vector) > MAX_SENTENCE_LENGTH:\n",
    "        #     sent_vector = sent_vector[:MAX_SENTENCE_LENGTH]\n",
    "        # else:\n",
    "        #     sent_vector = sent_vector + [[0] * 300] * (MAX_SENTENCE_LENGTH - len(sent_vector))\n",
    "\n",
    "        sentences_vector.append(sent_vector)\n",
    "    \n",
    "    return np.array(sentences_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_other_features(sentences):\n",
    "    vectors = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        vector = []\n",
    "\n",
    "        for token in nlp(sentence):\n",
    "            dictionary = {}\n",
    "\n",
    "            if not (token.is_stop or token.is_space or token.is_punct):\n",
    "                dictionary['pos'] = token.pos_\n",
    "                \n",
    "                associated_emotions = nrc_dict.get(token.lemma_, [])\n",
    "\n",
    "                for emotion in associated_emotions:\n",
    "                    dictionary[emotion] = True\n",
    "                \n",
    "                vector.append(dictionary)\n",
    "\n",
    "        if len(vector) > MAX_SENTENCE_LENGTH:\n",
    "            vector = vector[:MAX_SENTENCE_LENGTH]\n",
    "        else:\n",
    "            vector = vector + [{}] * (MAX_SENTENCE_LENGTH - len(vector))\n",
    "\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    print(len(vectors))\n",
    "\n",
    "    dict_vectorizer = DictVectorizer()\n",
    "    dict_vectorizer = dict_vectorizer.fit(list(itertools.chain.from_iterable(vectors)))\n",
    "\n",
    "    encoded = []\n",
    "\n",
    "    for vector in vectors:\n",
    "        encoded.append(dict_vectorizer.transform(vector).toarray())\n",
    "    \n",
    "    return np.array(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Stanford 140 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df_sf_train.text)[:250] + list(df_sf_train.text)[-250:]\n",
    "\n",
    "sentences = preprocess_tweets(sentences)\n",
    "sentences_embedded = encode_word2vec(sentences)\n",
    "features_embedded = encode_other_features(sentences)\n",
    "\n",
    "# Combine sentences_embedded and features_embedded on the third dimension\n",
    "combined_embedded = np.concatenate((sentences_embedded, features_embedded), axis=2)\n",
    "\n",
    "combined_embedded_2d = combined_embedded.reshape(combined_embedded.shape[0], combined_embedded.shape[1] * combined_embedded.shape[2]) # combined_embedded.reshape(combined_embedded.shape[0], -1)\n",
    "\n",
    "print(sentences_embedded.shape)\n",
    "print(features_embedded.shape)\n",
    "print(combined_embedded.shape)\n",
    "print(combined_embedded_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx = list(df_sf_train.text)[:10]\n",
    "testz = list(df_sf_train.text)[:10]\n",
    "testz = encode_word2vec(testz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenz(sentence):\n",
    "    return [word for word in nlp(sentence) if not (word.is_stop or word.is_space)]\n",
    "\n",
    "def caputed_seq(vectors):\n",
    "    output = []\n",
    "    for vector in vectors:\n",
    "        output.append(\"NO\" if list(vector) == [0] * 300 else \"YES\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "def summary(x, y):\n",
    "    a = tokenz(x)\n",
    "    b = caputed_seq(y)\n",
    "\n",
    "    for i in range(0, min(len(a), len(b))):\n",
    "        print(a[i], b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary(testx[5], testz[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_sf_train.polarity.to_list()[:250] + df_sf_train.polarity.to_list()[-250:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embedded_2d, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On GoSentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "sentences = list(df_gosentiment.text)[:60000]\n",
    "\n",
    "# sentences = preprocess_tweets(sentences)\n",
    "sentences_embedded = encode_word2vec(sentences)\n",
    "features_embedded = encode_other_features(sentences)\n",
    "\n",
    "# Combine sentences_embedded and features_embedded on the third dimension\n",
    "combined_embedded = np.concatenate((sentences_embedded, features_embedded), axis=2)\n",
    "\n",
    "combined_embedded_2d = combined_embedded.reshape(combined_embedded.shape[0], combined_embedded.shape[1] * combined_embedded.shape[2]) # combined_embedded.reshape(combined_embedded.shape[0], -1)\n",
    "\n",
    "print(sentences_embedded.shape)\n",
    "print(features_embedded.shape)\n",
    "print(combined_embedded.shape)\n",
    "print(combined_embedded_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_gosentiment.Polarity.to_list()[:60000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embedded_2d, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(solver='newton-cg', multi_class='ovr', max_iter=250)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "with open('LogisticRegression-60000.pkl', 'wb') as f:\n",
    "    pickle.dump(model)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_gosentiment.Polarity.to_list()[:60000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embedded_2d, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model_2 = SVC(kernel='linear')\n",
    "\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "with open('SVCModel-60000.pkl', 'wb') as f:\n",
    "    pickle.dump(model_2)\n",
    "\n",
    "pred_2 = model_2.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader = df_sf_train.sample(frac=0.01).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader.text = df_vader.text.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "#Add VADER metrics to dataframe\n",
    "df_vader['compound'] = [analyzer.polarity_scores(v)['compound'] for v in df_vader['text']]\n",
    "df_vader['neg'] = [analyzer.polarity_scores(v)['neg'] for v in df_vader['text']]\n",
    "df_vader['neu'] = [analyzer.polarity_scores(v)['neu'] for v in df_vader['text']]\n",
    "df_vader['pos'] = [analyzer.polarity_scores(v)['pos'] for v in df_vader['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRYPTO_PATHS = {\n",
    "    'Bitcoin': os.path.abspath('data/reddit-crypto/Bitcoin_12htop100_DailySub_0101_to_0817_PushShift_raw.csv'),\n",
    "    'Dogecoin': os.path.abspath('data/reddit-crypto/doge_12htop100_DailySub_0101_to_0710_PushShift.csv'),\n",
    "    'Solana': os.path.abspath('data/reddit-crypto/Solana_12htop100_DailySub_0101_to_0817_PushShift_raw.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv(CRYPTO_PATHS['Bitcoin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[reddit_df.selftext != '[removed]']\n",
    "\n",
    "reddit_df.title.fillna('', inplace=True)\n",
    "reddit_df.selftext.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of rows per day\n",
    "reddit_df.groupby('date', as_index=False)['title'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['fulltext'] = reddit_df.title + ': ' + reddit_df.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.fulltext = reddit_df.fulltext.apply(preprocess_reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty from fulltext\n",
    "reddit_df.fulltext = reddit_df.fulltext.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(reddit_df.fulltext)\n",
    "sentences_embedded = encode_word2vec(sentences)\n",
    "features_embedded = encode_other_features(sentences)\n",
    "\n",
    "# Combine sentences_embedded and features_embedded on the third dimension\n",
    "combined_embedded = np.concatenate((sentences_embedded, features_embedded), axis=2)\n",
    "\n",
    "combined_embedded_2d = combined_embedded.reshape(combined_embedded.shape[0], combined_embedded.shape[1] * combined_embedded.shape[2]) # combined_embedded.reshape(combined_embedded.shape[0], -1)\n",
    "\n",
    "print(sentences_embedded.shape)\n",
    "print(features_embedded.shape)\n",
    "print(combined_embedded.shape)\n",
    "print(combined_embedded_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sentiment = model.predict(combined_embedded_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['sentiment'] = list(reddit_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.to_csv(os.path.abspath('data/results/reddit_sentiment.csv'), sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df_recent = btc_df[-200000::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(btc_df_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'text' that are null\n",
    "btc_df_recent = btc_df_recent.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "sentences = list(btc_df_recent.text)\n",
    "\n",
    "sentences = preprocess_tweets(sentences)\n",
    "sentences_embedded = encode_word2vec(sentences)\n",
    "features_embedded = encode_other_features(sentences)\n",
    "\n",
    "# Combine sentences_embedded and features_embedded on the third dimension\n",
    "combined_embedded = np.concatenate((sentences_embedded, features_embedded), axis=2)\n",
    "\n",
    "# Save combined_embedded to pickle\n",
    "with open('combined_embedded_fdhskasdg.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_embedded, f)\n",
    "\n",
    "combined_embedded_2d = combined_embedded.reshape(combined_embedded.shape[0], combined_embedded.shape[1] * combined_embedded.shape[2]) # combined_embedded.reshape(combined_embedded.shape[0], -1)\n",
    "\n",
    "print(sentences_embedded.shape)\n",
    "print(features_embedded.shape)\n",
    "print(combined_embedded.shape)\n",
    "print(combined_embedded_2d.shape)\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_sentiments_recent = model.predict(combined_embedded_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df_recent['sentiment'] = list(btc_sentiments_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df_recent.to_csv('btc_df_recent.csv', sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preeprocessing idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_df[nrc_df['word'] == 'beautiful']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anger -> anger, annoyance\n",
    "anticipation\n",
    "disgust -> annoyance(?), disapproval, disgust\n",
    "fear -> embarrassment, fear, nervousness\n",
    "joy -> amusement, caring, excitement, gratitude, joy, love, optimism\n",
    "negative\n",
    "positive\n",
    "sadness -> disappointment, grief\n",
    "surprise -> realization\n",
    "trust -> admiration, approval\n",
    "\n",
    "none: confusion, curiosity, desire, pride\n",
    "\n",
    "relief\n",
    "remorse\n",
    "sadness\n",
    "surprise\n",
    "neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_goemotion.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "436a260425d2d1f41a03db831630dbba59f84586031c56b41ec9dd7831ec3a5e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('TextMining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
