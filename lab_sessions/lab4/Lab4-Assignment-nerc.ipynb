{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "import os\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader(os.path.abspath('CONLL2003'), 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "   a_dict = {\n",
    "      'words': token,\n",
    "      'pos': pos\n",
    "   }\n",
    "\n",
    "   training_features.append(a_dict)\n",
    "\n",
    "   training_gold_labels.append(ne_label)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_gold_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader(os.path.abspath('CONLL2003'), 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "        'words': token,\n",
    "        'pos': pos\n",
    "    }\n",
    "\n",
    "    test_features.append(a_dict)\n",
    "\n",
    "    test_gold_labels.append(ne_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in training set:  203621 203621\n",
      "Number of instances in test set:  46435 46435\n",
      "Frequency distribution of NERC labels\n",
      "(Training)\n",
      "{'O': 169578, 'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155}\n",
      "(Test)\n",
      "{'O': 38323, 'B-LOC': 1668, 'B-ORG': 1661, 'B-PER': 1617, 'I-PER': 1156, 'I-ORG': 835, 'B-MISC': 702, 'I-LOC': 257, 'I-MISC': 216}\n",
      "Frequency distribution adjusted for size of the sets for easy comparison purposes\n",
      "multiple 4.385075912565952\n",
      "(Training)\n",
      "{'O': 169578, 'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155}\n",
      "(Test)\n",
      "{'O': 168049.0, 'B-LOC': 7314.0, 'B-ORG': 7284.0, 'B-PER': 7091.0, 'I-PER': 5069.0, 'I-ORG': 3662.0, 'B-MISC': 3078.0, 'I-LOC': 1127.0, 'I-MISC': 947.0}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "print(\"Number of instances in training set: \", len(training_features), len(training_gold_labels))\n",
    "print(\"Number of instances in test set: \", len(test_features), len(test_gold_labels))\n",
    "\n",
    "counter_train = Counter(training_gold_labels)\n",
    "counter_test = Counter(test_gold_labels)\n",
    "\n",
    "counter_train_dict = dict(counter_train)\n",
    "counter_test_dict = dict(counter_test)\n",
    "\n",
    "# Sort counter_train_dict\n",
    "counter_train_dict = dict(sorted(counter_train_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "counter_test_dict = dict(sorted(counter_test_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"Frequency distribution of NERC labels\")\n",
    "print(\"(Training)\")\n",
    "print(counter_train_dict)\n",
    "print(\"(Test)\")\n",
    "print(counter_test_dict)\n",
    "\n",
    "print(\"Frequency distribution adjusted for size of the sets for easy comparison purposes\")\n",
    "\n",
    "multiple = len(training_gold_labels) / len(test_gold_labels)\n",
    "print(\"multiple\", multiple)\n",
    "\n",
    "counter_test_dict_adjusted = {k: round(v * multiple, 0) for k, v in counter_test_dict.items()}\n",
    "\n",
    "print(\"(Training)\")\n",
    "print(counter_train_dict)\n",
    "\n",
    "print(\"(Test)\")\n",
    "print(counter_test_dict_adjusted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of comparing how balanced the data is I multiplied the test dataset by a number so that the test and train set are the same proportion size thing for easy comparison purposes.\n",
    "\n",
    "\n",
    "In both the training and test set there are much more O labels than all other labels combined (but that is to be expected since most words aren't named entities)\n",
    "\n",
    "\n",
    "The other labels are not very balanced, they range from about 7300 instances having the label to about 1000.\n",
    "The three most common named entities (LOC, PER, ORG) are quite balanced, randing from about 7200 to 6300 in the training set and about 7300 to 7000 in the test set. The \"inner\" (I-) version of those named entities all have lower frequency and B-MISC and I-MISC have significantly lower frequency than the most common named entities.\n",
    "\n",
    "\n",
    "Between the training and test set the corresponding labels are balanced. They differ by at most around 15% and generally differ by around 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_set = training_features + test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "train_test_vector = vec.fit_transform(train_test_set).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector = train_test_vector[:len(training_features)]\n",
    "test_vector = train_test_vector[len(training_features):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf.fit(train_vector, training_gold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TextMining/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "##### Adapt the path to point to your local copy of the Google embeddings model\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(os.path.abspath('../../final-assignment/data/GoogleNews-vectors-negative300.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector_embedding = []\n",
    "train_gold_labels_embedding = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    if token == '' or token == 'DOCSTART':\n",
    "        continue\n",
    "\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0] * 300\n",
    "    \n",
    "    train_vector_embedding.append(vector)\n",
    "    train_gold_labels_embedding.append(ne_label)\n",
    "\n",
    "test_vector_embedding = []\n",
    "test_gold_labels_embedding = []\n",
    "\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    if token == '' or token == 'DOCSTART':\n",
    "        continue\n",
    "\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0] * 300\n",
    "    \n",
    "    test_vector_embedding.append(vector)\n",
    "    test_gold_labels_embedding.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf_embedding = svm.LinearSVC()\n",
    "lin_clf_embedding.fit(train_vector_embedding, train_gold_labels_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf_prediction = lin_clf.predict(test_vector)\n",
    "lin_clf_embedding_prediction = lin_clf_embedding.predict(test_vector_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without embedding\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.78      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.72       702\n",
      "       B-ORG       0.79      0.52      0.63      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.57      0.59      0.58       216\n",
      "       I-ORG       0.70      0.47      0.56       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.98      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.72      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Without embedding\")\n",
    "print(classification_report(test_gold_labels, lin_clf_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With embedding\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"With embedding\")\n",
    "print(classification_report(test_gold_labels_embedding, lin_clf_embedding_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models with and without embeddings are approximately equally good. Some metrics are higher in one and lower in the other model. Generally the metrics are within a few percentage points of each other. There is probably no statistically significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "kaggle_dataset = pd.read_csv(os.path.abspath('ner_dataset.csv'), encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_X = []\n",
    "training_Y = []\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    a_dict = {\n",
    "        'words': row['Word'],\n",
    "        'pos': row['POS']\n",
    "    }\n",
    "\n",
    "    training_X.append(a_dict)\n",
    "    training_Y.append(row['Tag'])\n",
    "\n",
    "\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    a_dict = {\n",
    "        'words': row['Word'],\n",
    "        'pos': row['POS']\n",
    "    }\n",
    "\n",
    "    test_X.append(a_dict)\n",
    "    test_Y.append(row['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_X = training_X + test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vectorizer = DictVectorizer()\n",
    "\n",
    "combined_X_vector = dict_vectorizer.fit_transform(combined_X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_X = combined_X_vector[:len(training_X)]\n",
    "test_X = combined_X_vector[len(training_X):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_svm = svm.LinearSVC()\n",
    "lin_svm.fit(training_X, training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lin_svm.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         4\n",
      "       B-eve       0.00      0.00      0.00         0\n",
      "       B-geo       0.80      0.76      0.78       741\n",
      "       B-gpe       0.96      0.92      0.94       296\n",
      "       B-nat       1.00      0.50      0.67         8\n",
      "       B-org       0.64      0.51      0.57       397\n",
      "       B-per       0.81      0.53      0.64       333\n",
      "       B-tim       0.91      0.76      0.83       393\n",
      "       I-art       0.00      0.00      0.00         0\n",
      "       I-eve       0.00      0.00      0.00         0\n",
      "       I-geo       0.74      0.50      0.60       156\n",
      "       I-gpe       1.00      0.50      0.67         2\n",
      "       I-nat       0.80      1.00      0.89         4\n",
      "       I-org       0.65      0.44      0.53       321\n",
      "       I-per       0.42      0.90      0.57       319\n",
      "       I-tim       0.41      0.08      0.14       108\n",
      "           O       0.98      0.99      0.99     16918\n",
      "\n",
      "    accuracy                           0.94     20000\n",
      "   macro avg       0.60      0.49      0.52     20000\n",
      "weighted avg       0.95      0.94      0.94     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TextMining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/TextMining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/TextMining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/TextMining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/TextMining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/TextMining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_Y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of the NERC labels have 0.0 on all metrics, probably because those labels aren't present in the test set.\n",
    "\n",
    "The weighted average is high because the precision, recall and F1 score for the label O is high and that class is highly overrepresented, so the macro average is a more true measure of the model's performance.\n",
    "\n",
    "The macro average precision is 0.6, and the macro aveerage recall and F1 score are around 0.5. Overall that's not very good but it's not terrible.\n",
    "\n",
    "The different metrics for the different labels are all over the place. They vary greatly from label to label and within a label the precision, recall and F1 score are often very different.\n",
    "\n",
    "For example B-gpe has above 90% for all three metrics, and B-org has around 60% for all three metrics.\n",
    "I-per has 0.41 precision and 0.90 recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
